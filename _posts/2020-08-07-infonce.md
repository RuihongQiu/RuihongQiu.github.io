---
title: 'Explanation of Contrastive Predictive Coding'
date: 2020-08-07
permalink: /posts/2020/08/infonce/
tags:
  - mutual information
topic: 'mi'
---

Explanation of the paper *Representing Learning with Contrastive Predictive Coding, arXiv 2019*.

# 1. Introduction

This paper is kind of parallel to the previous [MINE](https://ruihongqiu.github.io/posts/2020/07/mine/){:target="_blank"} method. This paper also provides an objective to optimize the mutual information between random variables. This InfoNCE objective is later discussed and claimed to be tighter than MINE.

Let's begin with the this paper, [Representing Learning with Contrastive Predictive Coding](https://arxiv.org/abs/1807.03748){:target="_blank"}, in arXiv.

# 2. Method Detail

We will discuss from the mutual information itself to how to get an objective function. If there is any concern about the notation and the model structure, please referred to Figure 1 of the original paper.

## 2.1. Mutual Information

In the MINE paper, the mutual information is represented as a KL term, which later is transformed into the DV representation. While in this InfoNCE method, the MI between the original signal $$x$$ and the encoding $$c$$ is firstly defined as:

$$\label{eq:mi}
I(x ; c)=\sum_{x, c} p(x, c) \log \frac{p(x \mid c)}{p(x)}.
$$

## 2.2. Contrastive Predictive Coding

The original task is a sequence prediction task. Given observed input $$x_t$$, an encoder maps the input into a sequential latent representation $$z_t=g_\text{enc}(x_t)$$. And an autoregressive model summarizes all $$z_{\leq t}$$ into a context latent representation $$c_{t}=g_{\text{ar}}\left(z_{\leq t}\right)$$.

To predict the futural signals $$x_{t+k}$$, we do not apply a generative model $$p_k(x_{t+k}\mid c)$$. Instead, we want to maximize the MI between $$x_{t+k}$$ and $$x_{\leq t}$$ (or $$c_t$$). According to Eq. (\ref{eq:mi}), the ground truth joint distribution $$p(x,c)$$ is not the thing that we can control, we want to maximize the density ratio:

$$\label{eq:ratio}
f_{k}\left(x_{t+k}, c_{t}\right) \propto \frac{p\left(x_{t+k} \mid c_{t}\right)}{p\left(x_{t+k}\right)},
$$

where we can do a "proportional to" trick because it is an unbounded value. In the paper, this density ratio is approximated by the following function:

$$\label{eq:nn}
f_{k}\left(x_{t+k}, c_{t}\right)=\exp \left(z_{t+k}^{T} W_{k} c_{t}\right),
$$

where $$W_k$$ is for the $$k$$-th step. This function is similar to a dot product, which could indicate the similarity between the predicted latent representation $$z_{t+k}^{T}$$ and the step-wise context latent representation $$W_{k} c_{t}$$.

## 2.3. InfoNCE Loss

After obtaining a surrogate representation function to the MI, we want to plug it into the loss function. Here, the NCE loss is applied:

$$\label{eq:loss}
\mathcal{L}_{\mathrm{N}}=-\underset{X}{\mathbb{E}}\left[\log \frac{f_{k}\left(x_{t+k}, c_{t}\right)}{\sum_{x_{j} \in X} f_{k}\left(x_{j}, c_{t}\right)}\right],
$$

where $$X$$ is a set of samples, $$X=\left\{x_{1}, \ldots x_{N}\right\}$$. Using this set, the $$x_{t+k}$$ is generated from $$c_t$$, which can be viewed as a positive sample pair. While for all other samples $$x_j\in X$$, along with the same $$c_t$$, they serve as negative samples. To minimize this loss, intuitively, we will have the numerator as large as possible while the denominator as small as possible. This is exactly what we want: the MI between positive samples to be large while for negative samples, to be small.

## 2.4. Estimating MI with InfoNCE


