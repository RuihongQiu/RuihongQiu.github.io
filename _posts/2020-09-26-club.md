---
title: 'Explanation of CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information'
date: 2020-09-26
permalink: /posts/2020/09/club/
tags:
  - mutual information
topic: 'mi'
---

Explanation of the paper *CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information, ICML 2020*.

# 1. Introduction

This paper introduces a contrastive log-ratio upper bound of the mutual information. It provides a more stable estimation than the previously proposed L1OUT upper bound [(previous post)](https://ruihongqiu.github.io/posts/2020/09/vbmi/){:target="_blank"}.

Let's begin with this paper, [CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information](https://arxiv.org/abs/2006.12013){:target="_blank"}, in ICML 2020.

# 2. Baseline Upper Bounds

In our [previous post about variational bounds](https://ruihongqiu.github.io/posts/2020/09/vbmi/){:target="_blank"}, we discuss two types of upper bounds.

## 2.1. $$\mathrm{I}_{\mathrm{VUB}}$$

Alemi et al. (2016) introduces a variational marginal approximation $$r(y)$$ to build a variational upper bound (VUB):

$$
\begin{aligned}
\mathrm{I}(\boldsymbol{x} ; \boldsymbol{y}) &=\mathbb{E}_{p(\boldsymbol{x}, \boldsymbol{y})}\left[\log \frac{p(\boldsymbol{y} \mid \boldsymbol{x})}{p(\boldsymbol{y})}\right] \\
&=\mathbb{E}_{p(\boldsymbol{x}, \boldsymbol{y})}\left[\log \frac{p(\boldsymbol{y} \mid \boldsymbol{x})}{r(\boldsymbol{y})}\right]-\operatorname{KL}(p(\boldsymbol{y}) \| r(\boldsymbol{y})) \\
& \leq \mathbb{E}_{p(\boldsymbol{x}, \boldsymbol{y})}\left[\log \frac{p(\boldsymbol{y} \mid \boldsymbol{x})}{r(\boldsymbol{y})}\right]=\mathrm{KL}(p(\boldsymbol{y} \mid \boldsymbol{x}) \| r(\boldsymbol{y})) = \mathrm{I}_{\mathrm{VUB}}.
\end{aligned}
$$

In the experiment, the variational distribution $$r(y)$$ is usually set to a standard normal distribution, which results in the estimation of MI to be high-biased.

## 2.2. $$\mathrm{I}_{\mathrm{L1Out}}$$

Poole et al. (2019) [(previous post)](https://ruihongqiu.github.io/posts/2020/09/vbmi/){:target="_blank"} replaces $$r(y)$$ with a Monte-Carlo approximation $$r_{i}(\boldsymbol{y})=\frac{1}{N-1} \sum_{j \neq i} p\left(\boldsymbol{y} \mid \boldsymbol{x}_{j}\right) \approx p(\boldsymbol{y})$$ and derives a leave one-out upper bound (L1Out):

$$
\mathrm{I}_{\mathrm{L} 1 \mathrm{Out}}:=\mathbb{E}\left[\frac{1}{N} \sum_{i=1}^{N}\left[\log \frac{p\left(\boldsymbol{y}_{i} \mid \boldsymbol{x}_{i}\right)}{\frac{1}{N-1} \sum_{j \neq i} p\left(\boldsymbol{y}_{i} \mid \boldsymbol{x}_{j}\right)}\right]\right].
$$

## 2.3. Unknown Conditional Distribution

When the conditional distribution is unknown (not like VAE), we will introduce a neural network $$q_\theta(y\mid x)$$ to approximate $$p(y\mid x)$$ and develop variational versions of VUB and L1Out as:

$$
\mathrm{I}_{\mathrm{vVUB}}=\mathbb{E}_{p(\boldsymbol{x}, \boldsymbol{y})}\left[\log \frac{q_{\theta}(\boldsymbol{y} \mid \boldsymbol{x})}{r(\boldsymbol{y})}\right],
$$

$$
\mathrm{I}_{\mathrm{VL} 1 \mathrm{Out}}=\mathbb{E}\left[\frac{1}{N} \sum_{i=1}^{N}\left[\log \frac{q_{\theta}\left(\boldsymbol{y}_{i} \mid \boldsymbol{x}_{i}\right)}{\frac{1}{N-1} \sum_{j \neq i} q_{\theta}\left(\boldsymbol{y}_{i} \mid \boldsymbol{x}_{j}\right)}\right]\right].
$$
