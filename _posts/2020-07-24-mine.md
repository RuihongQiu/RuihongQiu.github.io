---
title: 'Mutual Information Neural Estimation'
date: 2020-07-24
permalink: /posts/2020/07/mine/
tags:
  - mutual information
topic: 'mi'
---

Explanation of Mutual Infomation Neural Estimation, ICML 2018.

# Mutual Infomation Neural Estimation

Mutual information has been successfully applied in deep learning recently. The original difficulty of using mutual information is that it is hard to compute exactly. Recent methods focus on how to derive a tractable bound that can be optimized on.

Let's begin with the first big-hit paper, [Mutual Information Neural Estimation](https://arxiv.org/abs/1801.04062), in ICML2018.

## Mutual Information

Mutual information $$I$$ quantifies the dependence of two random variables $$X$$ and $$Z$$ (can be think of as input variables and the corresponding latent variables). $$I(X;Z)$$ is defined as:

$$
I(X;Z)=H(X)-H(X|Z)=H(Z)-H(Z|X)=I(Z;X),
$$

where $$H$$ is Shannon entropy of a random variable. $$H(X)=-\mathbb(E)_{p(x)}\lg p(x)$$
