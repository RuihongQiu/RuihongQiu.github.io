---
title: 'Mutual Information Neural Estimation'
date: 2020-07-24
permalink: /posts/2020/07/mine/
tags:
  - mutual information
topic: 'mi'
---

Explanation of Mutual Infomation Neural Estimation, ICML 2018.

# Mutual Infomation Neural Estimation

Mutual information has been successfully applied in deep learning recently. The original difficulty of using mutual information is that it is hard to compute exactly. Recent methods focus on how to derive a tractable bound that can be optimized on.

Let's begin with the first big-hit paper, [Mutual Information Neural Estimation](https://arxiv.org/abs/1801.04062), in ICML2018.

## Mutual Information

Mutual information $$I$$ quantifies the statistical dependence of two random variables $$X$$ and $$Z$$ (can be thought of as input variables and the corresponding latent variables). $$I(X;Z)$$ is generally defined as:

$$
I(X;Z)=H(X)-H(X\mid Z)=H(Z)-H(Z\mid X)=I(Z;X),
$$

where $$H$$ is Shannon entropy of a random variable. $$H(X)=-\mathbb{E}_{p(x)}\log p(x)$$ and $$H(X\mid Z)=--\mathbb{E}_{p(x,z)}\log p(x\mid z)$$.

Note that $$H(X\mid Z)$$ is derived as:

$$
\begin{aligned}
H(X\mid Z)&=\sum_{z\in\mathcal{Z}}p(z)H(X\mid Z=z) \\
&=-\sum_{z\in\mathcal{Z}}p(z)\sum_{x\in\mathcal{X}}p(x\mid z)\log p(x\mid z) \\
&=-\sum_{x\in\mathcal{X},z\in\mathcal{Z}}p(x,z)\log p(x\mid z).
\end{aligned}
$$

There is a more useful way, in terms of computation, to describe $$I(X;Z)$$:

$$
\begin{aligned}
I(X; Z) &=H(X)-H(X \mid Z) \\
&=-\int_{x} p(x) \log p(x) d x+\int_{x, z} p(x, z) \log p(x \mid z) d x d z \\
&=\int_{x, z}(-p(x, z) \log p(x)+p(x, z) \log p(x \mid z)) d x d z \\
&=\int_{x, z}\left(p(x, z) \log \frac{p(x, z)}{p(x) p(z)}\right) d x d z \\
&=D_{K L}(P(X, Z) \| P(X) \otimes P(Z))
\end{aligned}
$$

So the mutual information can be thought of as the KL divergence between the joint distribution and the product of both marginal distributions. And the mutual information is obviously sysmetric.

According to the KL divergence form of the mutual information, if $$X$$ and $$Z$$ are independent, $$p(x, z) = p(x) \times p(z)$$ and $$I(X;Z)=0$$. If $$X$$ and $$Z$$ become more dependent, the mutual information will intuitively increase. Therefore, mutual information can be considered as a good metric for determining the dependence between random variables.

## The Donsker-Varadhan representation of KL

Although we have a metric for the dependence, this KL form is intractable (yet). Consider an normal encoder model, we will only have $$P(Z\mid X)$$. While $$P(X,Z)$$, $$P(X)$$ and $$P(Z)$$ are all intractable. Therefore, we need to find a tractable representation to optimize. Here comes the Donsker-Varadhan (DV) representation.
