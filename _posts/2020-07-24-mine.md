---
title: 'Mutual Information Neural Estimation'
date: 2020-07-24
permalink: /posts/2020/07/mine/
tags:
  - mutual information
topic: 'mi'
---

Explanation of Mutual Infomation Neural Estimation, ICML 2018.

# Mutual Infomation Neural Estimation

Mutual information has been successfully applied in deep learning recently. The original difficulty of using mutual information is that it is hard to compute exactly. Recent methods focus on how to derive a tractable bound that can be optimized on.

Let's begin with the first big-hit paper, [Mutual Information Neural Estimation](https://arxiv.org/abs/1801.04062), in ICML2018.

## Mutual Information

Mutual information $$I$$ quantifies the dependence of two random variables $$X$$ and $$Z$$ (can be think of as input variables and the corresponding latent variables). $$I(X;Z)$$ is generally defined as:

$$
I(X;Z)=H(X)-H(X\mid Z)=H(Z)-H(Z\mid X)=I(Z;X),
$$

where $$H$$ is Shannon entropy of a random variable. $$H(X)=-\mathbb{E}_{p(x)}\log p(x)$$ and $$H(X\mid Z)=-E_{p(x,z)}\log p(x\mid z)$$.

Note that $$H(X\mid Z)$$ is derived as:

$$
\begin{aligned}
H(X\mid Z)&=\sum_{z\in\mathcal{Z}}p(z)H(X\mid Z=z)
&=-\sum_{z}p(z)\sum_{x\in\mathcal{X}}p(x\mid z)\log p(x\mid z)
&=-\sum_{x\in\mathcal{X},z\in\mathcal{Z}}p(x,z)\log p(x\mid z).
\end{aligned}
$$

There is a more useful way, in terms of computation, to describe $$I(X;Z)$$:

$$
\begin{aligned}
I(X; Z) &=H(X)-H(X \mid Z) \\
&=-\int_{x} p(x) \log p(x) d x+\int_{x, z} p(x, z) \log p(x \mid z) d x d z \\
&=\int_{x, z}(-p(x, z) \log p(x)+p(x, z) \log p(x \mid z)) d x d z \\
&=\int_{x, z}\left(p(x, z) \log \frac{p(x, z)}{p(x) p(z)}\right) d x d z \\
&=D_{K L}(P(X, Z) \| P(X) \otimes P(Z))
\end{aligned}
$$

So the mutual information can be thought of as the KL divergence between the joint distribution and the product of both marginal distributions.
